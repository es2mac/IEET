## Learning from Evaluation: David Reider

David Reider, Principal Partner at [Education Design](http://www.educationdesign.biz)


### What evaluators do

- Projects typically have objectives and impact goals

- Program evaluation: to weigh in on the quality of the project

- Sometimes clients just want thumbs up/down, but education is complex

- Evaluators try to be objective and have no vested interest

- Evaluation vs. assessment: Research is hypothesis based, evaluation is purely
  emergent (without hypothesis)

- Like Rashomon, tech/teacher/student/researchers may all have a different
  perspective, and evaluator tries to tell the truth


### Formative and summative evaluation

- Summative evaluation: at the end.  Often compliance-based and not helpful

- Formative evaluation: oftentimes projects change in the middle and wonderful
  things happen, and evaluators need to / help calibrate with the team


### Stakeholders

Evaluators wear many hats because each stakeholder is a different audience.

- Large funded projects require evaluators to write report, but sometimes it
  doesn't help the field

- Teachers tend to get reports at a later time frame, relevance is gone

- Decision makers: curriculum director, principals and up to state-level,
  sometimes vote based on evaluations

- Presenting / having a dialog communicates better than a long report


### Phases of evaluation

Working with school often dictates phases corresponding to a school year.

- Beginning: new product, design the points of intervention, instruments
  (surveys, ways to collect and analyze data)

- Before intervention: get baseline data

- During intervention: observation in the field

- At the end: follow-up post data

- After: analyze data an report


### Reflection for teachers

The very nature of asking questions serves as reflection for the teachers
(which teachers ofter don't have time for)

- Teachers always underestimates their effect, their skills and their importance

- Often evaluators have a chance to bring along experience of other teachers,
  cross-pollinating ideas

- Teacher themselves may not be able to do this (self-evaluate) and tend to
  only able to look at what they know (content things), but an outside person
  or someone like a technology coordinator could


### Example of Next-Generation Preschool Math

- In the process they discovered interesting things, such as high technology
  penetration in the disadvantaged areas

- Evaluation guided next-step program developments


### Between developers and researchers

- Two very established groups found their goals clash in interesting ways
    - Developers: make interesting games
    - Researchers: need to study methodically

- These come about from evaluators' investigations


### Areas evaluators focus on

- What to evaluate on, which requires a lot of careful thought

- Usability, for the students and the teachers

- Access

- Is technology allowing deeper ways of learning
    - Moving towards constructivist/constructionist
    - Teachers need to know the indicators of deeper learning


### On scores

- In David's career, not once has students or classroom scored lower than how
  they did previously

- Scores tend to slightly rise, not statistically strong, but students are
  additionally learning computational thinking, group work... it's an
  indisputable win


### Technology for the evaluation practice

- David do a lot of qualitative stuff, and uses technology no more than other
  technology-rich environments

- Data collection did get easier, but right now can't do a lot with it

